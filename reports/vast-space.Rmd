---
title: "Exploring S and N space"
output: github_document
date: 10/8/2019
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(drake)
library(dplyr)
library(ggplot2)
library(scads)
full <- FALSE
```

## Combinations of S & N

These are the combinations of S and N I ran sampling on. There are three chunks: small, large S, and large N. Small is to get at behavior for very small S and N. Large S and large N explore larger communities in those directions. The bottom right corner is gently constrained because N must be >= to S. I tried to explore the general regions of space filled by the datasets in the Baldridge/White papers. I've done this edges-approach because is computationally expensive to explore the large S-large N corner, and there aren't datasets there. 

```{r S and N combos}

small_s <- c(2:10)
small_n <-  c(2:19, seq(20, 200, by = 10))

small_combinations <- expand.grid(S = small_s, N = small_n) %>%
  dplyr::filter(N > S) %>%
  filter(N %in% seq(4, 200, by = 4)) %>%
  filter(N %in% c(0:40, 100, 140, 160, 200))


large_s <- c(10, 20, 30, seq(50, 250, by = 50))
moderate_n <- seq(50, 250, by = 50)

ls_mn_combinations <- expand.grid(S = large_s, N = moderate_n) %>%
  dplyr::filter(N > S)

moderate_s <- c(seq(5, 40, by = 5), 50, 75)
large_n <- c(seq(500, 2500, by = 500), 5000, 7500)

ms_ln_combinations <- expand.grid(S = moderate_s, N = large_n) %>%
  dplyr::filter(N > S)

all_combinations <- dplyr::bind_rows(small = small_combinations, large_s = ls_mn_combinations, large_n= ms_ln_combinations, .id = "source") %>%
  mutate(source = as.factor(source))

combinations_plot <- ggplot(data = all_combinations, aes(x = S, y = N, color = source)) +
  geom_point() +
  theme_bw() +
  scale_color_viridis_d(end = .8)

combinations_plot

combinations_plots_zoomed <- combinations_plot +
  facet_wrap(~source, strip.position = "top", scales = "free")
combinations_plots_zoomed
```

```{r cached objects manip, eval = F}

## Set up the cache and config
db <- DBI::dbConnect(RSQLite::SQLite(), here::here("drake", "drake-cache.sqlite"))
cache <- storr::storr_dbi("datatable", "keystable", db)

cached_fs <- cached(cache=cache) [which(substr(cached(cache=cache), 0, 2) == "fs")]
fs_S <- vapply(as.matrix(cached_fs), FUN = function(aword) return(as.integer(unlist(strsplit(aword, "_")[[1]][[2]]))), FUN.VALUE = 10)
fs_N <- vapply(as.matrix(cached_fs), FUN = function(aword) return(as.integer(unlist(strsplit(aword, "_")[[1]][[3]]))), FUN.VALUE = 10)

fs_lookup <- data.frame(sn_combination = cached_fs, S = fs_S, N = fs_N, stringsAsFactors = F)

cached_di <- cached(cache=cache) [which(substr(cached(cache=cache), 0, 2) == "di")]
di_S <- vapply(as.matrix(cached_di), FUN = function(aword) return(as.integer(unlist(strsplit(aword, "_")[[1]][[3]]))), FUN.VALUE = 10)
di_N <- vapply(as.matrix(cached_di), FUN = function(aword) return(as.integer(unlist(strsplit(aword, "_")[[1]][[4]]))), FUN.VALUE = 10)
di_lookup <- data.frame(sn_combination = cached_di, S = di_S, N = di_N, stringsAsFactors = F)


fs_list <- lapply(as.matrix(cached_fs), FUN = readd, character_only = T, cache = cache)
names(fs_list) <- cached_fs
di_list <- lapply(as.matrix(cached_di), FUN = readd, character_only = T, cache = cache)
names(di_list) <- cached_di

fs_df <- bind_rows(fs_list, .id = "sn_combination") %>%
  left_join(fs_lookup, by = "sn_combination")
di_df <- bind_rows(di_list, .id = "sn_combination") %>%
  left_join(di_lookup, by = "sn_combination")

rm(fs_list)
rm(di_list)
rm(cached_fs)
rm(cached_di)

write.csv(fs_df, file = here::here("drake", "fs_df.csv"), row.names= F)

fs_df_1000 <- filter(fs_df, sim <= 1000)

write.csv(fs_df_1000, file = here::here("drake", "fs_df_first1000.csv"), row.names= F)


write.csv(di_df, file = here::here("drake", "di_df.csv"), row.names = F)


```



## Number of unique elements (out of 10,000 draws)

I've drawn 10,000 times from the feasible set for each of these S-N combinations. For some of them, we won't be able to get 10,000 unique elements from the feasible set because the FS is in fact much smaller than that. 

```{r unique draws}
if(full) {
di_df <- read.csv(here::here("drake", "di_df.csv"), stringsAsFactors = F)
} else {
  di_df <- read.csv(here::here("drake", "di_df_first1000.csv"), stringsAsFactors = F)

}
ndraws <- di_df %>%
  select(S, N, sim) %>%
  distinct() %>%
  group_by(S, N) %>%
  summarize(nsamples = n()) %>%
  ungroup() %>%
  mutate(log_nsamples = log(nsamples)) %>%
  inner_join(all_combinations, by = c("S", "N"))

ndraws_plot <- ggplot(data = ndraws, aes(x = S, y = N, color = log_nsamples)) +
  geom_point() +
  theme_bw() +
  scale_color_viridis_c(end = .8, direction = -1, option = "magma")

ndraws_plot

ndraws_plot_zoomed <- ndraws_plot +
  facet_wrap(~source, strip.position = "top", scales = "free")
ndraws_plot_zoomed


```

## Shapes represented - violins

Generally, for the small combinations, it looks like

* the mean value of whatever index increases with increasing S (2:10)
* the lower tail of the index gets longer with increasing N (4:200)
* the tails taper more sharply for Shannon & Simpson than for skewness
* Skewness seems to behave a little counterintuitively. It appears to _increase_ with increasing S; very low S has near-0 skewness. For a given S, the mean skewness decreases with increasing N, and the tail gets longer. Within the scope of strictly increasing RADs, *the larger the skewness, the longer the rare tail in the SAD*. 

```{r violins, fig.width = 12, fig.height = 12} 

di_df_M <- di_df %>%
  select(-source) %>%
  inner_join(all_combinations, by = c("S", "N")) %>%
  tidyr::gather(-sn_combination, -source, -S, -N, -sim, key = "variable", value = "val") %>%
  mutate(nspp = as.factor(S),
         nind = as.factor(N)) %>%
  group_by(S, N, variable) %>%
  mutate(col_val = mean(val, na.rm = T)) %>%
  ungroup() %>%
  group_by(S, N) %>%
  mutate(alpha_val = max(sim) / 10000) 


skew_violins <- ggplot(data = filter(di_df_M, variable == "skew", S > 2), aes(x = 0, y = val, color = col_val, fill = col_val, alpha = alpha_val)) +
  geom_violin() +
 # geom_point(data = filter(di_df_M, variable == "skew", S == 2)) +
  theme_bw() +
  theme(panel.grid = element_blank(),
        axis.ticks.x = element_blank(),
        axis.text.x = element_blank(),
        panel.spacing = unit(.001, "lines"),
        strip.text = element_text(size = 12)) +
  facet_grid(rows = vars(nind), cols = vars(nspp), switch = "both", scales = "fixed", as.table = F) +
  scale_color_viridis_c(option = "plasma", end = .8) +
   scale_fill_viridis_c(option = "plasma", end = .8) +
  ggtitle("Skew violins")

suppressWarnings(print(skew_violins))


shannon_violins <- ggplot(data = filter(di_df_M, variable == "shannon"), aes(x = 0, y = val, color = col_val, fill = col_val, alpha = alpha_val)) +
  geom_violin() +
 geom_point(data = filter(di_df_M, variable == "skew", S == 2)) +
  theme_bw() +
  theme(panel.grid = element_blank(),
        axis.ticks.x = element_blank(),
        axis.text.x = element_blank(),
        panel.spacing = unit(.001, "lines"),
        strip.text = element_text(size = 12)) +
  facet_grid(rows = vars(nind), cols = vars(nspp), switch = "both", scales = "fixed", as.table = F) +
  scale_color_viridis_c(option = "plasma", end = .8) +
   scale_fill_viridis_c(option = "plasma", end = .8) +
  ggtitle("Shannon violins")

suppressWarnings(print(shannon_violins))


simpson_violins <- ggplot(data = filter(di_df_M, variable == "simpson"), aes(x = 0, y = val, color = col_val, fill = col_val, alpha = alpha_val)) +
  geom_violin() +
geom_point(data = filter(di_df_M, variable == "skew", S == 2)) +
  theme_bw() +
  theme(panel.grid = element_blank(),
        axis.ticks.x = element_blank(),
        axis.text.x = element_blank(),
        panel.spacing = unit(.001, "lines"),
        strip.text = element_text(size = 12)) +
  facet_grid(rows = vars(nind), cols = vars(nspp), switch = "both", scales = "fixed", as.table = F) +
  scale_color_viridis_c(option = "plasma", end = .8) +
   scale_fill_viridis_c(option = "plasma", end = .8) +
  ggtitle("Simpson violins")

suppressWarnings(print(simpson_violins))

```


```{r dotplots, fig.height = 50, fig.width =4}
simpson_dotplots <- ggplot(data = filter(di_df_M, variable == "simpson"), aes(x = N, y = val)) +
geom_point(alpha = .01) +
  theme_bw() +
  theme(panel.grid = element_blank(),
        panel.spacing = unit(.001, "lines")#,
        #strip.text = element_text(size = 12)) 
        ) +
  facet_wrap(vars(nspp), scales = "free_y", ncol = 1, strip.position = "top") +
  ggtitle("Simpson dotplots")

simpson_dotplots


skew_dotplots <- ggplot(data = filter(di_df_M, variable == "skew"), aes(x = N, y = val)) +
geom_point(alpha = .01) +
  theme_bw() +
  theme(panel.grid = element_blank(),
        panel.spacing = unit(.001, "lines")#,
        #strip.text = element_text(size = 12)) 
        ) +
  facet_wrap(vars(nspp), scales = "free_y", ncol = 1, strip.position = "top") +
  ggtitle("skew dotplots")

skew_dotplots
```

Q: Why, for a given number of species, does adding individuals increase diversity and decrease skewness?
A: The diversity indices are definitely sensitive to N. I'm not sure about skewness. 

## Heatmaps

```{r prep heatmap data, fig.width = 15, fig.height = 15}

if(full) {
fs_df <- read.csv(here::here("drake", "fs_df.csv"), stringsAsFactors = F)
} else {
  fs_df <- read.csv(here::here("drake", "fs_df_first1000.csv"), stringsAsFactors = F)
}

fs_df_M <- fs_df %>%
  select(-source) %>%
  filter(!(grepl(sn_combination, pattern = "table_2"))) %>%
  mutate(sn_combination = paste0("di_", sn_combination)) %>%
  left_join(di_df, by = c("S", "N", "sim", "sn_combination")) %>%
  select(-sn_combination, -source) %>%
  left_join(all_combinations, by = c("S", "N")) %>%
  mutate(nind = as.factor(N),
         nspp = as.factor(S)) %>%
  filter(!is.na(source))

skew_heatmaps <- list()

combos <- unique(fs_df_M$source)

skewlims <- c(min(fs_df_M$skew, na.rm = T), max = max(fs_df_M$skew, na.rm = T))

for(c in combos) {
skew_heatmaps$c <- ggplot(data = filter(fs_df_M, source == c), aes(x = rank, y = abund, color = skew, group = sim)) +
  geom_line(alpha = .01) +
  theme_bw() +
  theme(panel.grid = element_blank(),
        axis.ticks.x = element_blank(),
        axis.text.x = element_blank(),
        panel.spacing = unit(.001, "lines"),
        strip.text = element_text(size = 12)) +
  facet_grid(rows = vars(nind), cols = vars(nspp), switch = "both", scales = "free", as.table = F) +
  scale_color_viridis_c(option = "plasma", end = .8, limits = skewlims) +
  ggtitle(paste0("Skew heatmaps - ", c))
names(skew_heatmaps)[ which(names(skew_heatmaps) == "c")] <- c
}

for(i in 1:length(combos)) {
  suppressWarnings(print(skew_heatmaps[[i]]))
}


```


The most skewed - by a lot - panel appears to be S = 200, N = 250. Zooming in on that one....


```{r S 200 N 250}

s200_n250_heatmap <- ggplot(data = filter(fs_df_M, S == 200, N == 250), aes(x = rank, y = abund, color = skew, group = sim)) +
  geom_line(alpha = .1) +
  theme_bw() +
  scale_color_viridis_c(option = "plasma", end = .8, limits = skewlims) +
  ggtitle("Skew heatmap - S = 200; N = 250")

s200_n250_heatmap

```

```{r more heatmaps, fig.height = 15, fig.width = 15}

shannon_heatmaps <- list()

shannonlims <- c(min(fs_df_M$shannon, na.rm = T), max = max(fs_df_M$shannon, na.rm = T))

for(c in combos) {
shannon_heatmaps$c <- ggplot(data = filter(fs_df_M, source == c), aes(x = rank, y = abund, color = shannon, group = sim)) +
  geom_line(alpha = .01) +
  theme_bw() +
  theme(panel.grid = element_blank(),
        axis.ticks.x = element_blank(),
        axis.text.x = element_blank(),
        panel.spacing = unit(.001, "lines"),
        strip.text = element_text(size = 12)) +
  facet_grid(rows = vars(nind), cols = vars(nspp), switch = "both", scales = "free", as.table = F) +
  scale_color_viridis_c(option = "plasma", end = .8, limits = shannonlims) +
  ggtitle(paste0("Shannon heatmaps - ", c))
names(shannon_heatmaps)[ which(names(shannon_heatmaps) == "c")] <- c
}

for(i in 1:length(combos)) {
  suppressWarnings(print(shannon_heatmaps[[i]]))
}


simpson_heatmaps <- list()
simpsonlims <- c(min(fs_df_M$simpson, na.rm = T), max = max(fs_df_M$simpson, na.rm = T))

for(c in combos) {
simpson_heatmaps$c <- ggplot(data = filter(fs_df_M, source == c), aes(x = rank, y = abund, color = simpson, group = sim)) +
  geom_line(alpha = .01) +
  theme_bw() +
  theme(panel.grid = element_blank(),
        axis.ticks.x = element_blank(),
        axis.text.x = element_blank(),
        panel.spacing = unit(.001, "lines"),
        strip.text = element_text(size = 12)) +
  facet_grid(rows = vars(nind), cols = vars(nspp), switch = "both", scales = "free", as.table = F) +
  scale_color_viridis_c(option = "plasma", end = .8, limits = simpsonlims) +
  ggtitle(paste0("Simpson heatmaps - ", c))
names(simpson_heatmaps)[ which(names(simpson_heatmaps) == "c")] <- c
}

for(i in 1:length(combos)) {
  suppressWarnings(print(simpson_heatmaps[[i]]))
}

```

After staring at these.

1. Shannon is irritating because it contains infromation about both richness and diversity; across large gradients in S, it becomes very difficult to understand how changes in the distribution of Shannon values reflects a) changing shapses and b)just the change in S.
2. Skewness tells you something about how far to the right the J is. If you have a lot of extra individuals to smudge over few species, you can achieve relatively low skewness values vs. when S approaches N, and you can only have a few FS and they all have the J pretty far to the right. This is why the only really high skew values are in the large S plots, as S approaches N. 
3. Simpson is tracking a combination of how far rigth and how *tall* the J is. More individuals gives you higher Simpson values (generallly), because they create the capacity for really tall J's. But adding species *also* increases the simpson's values, I think because they remove the Js that are tall, yes, but also have more elements where the excess N are smudged over more of the species. For an N, increasing S hollows out the upper left/inner elbow of the J.
4. The overall takeaways are...
    1. Shannon's is a mind trip and not super helpful.
    2. Simpson's and skewness tell us different things; they're neither completely orthogonal nor completely correlated. 
    3. The underlying statistical constraint's shape is contingent on S and N and their ratio; we expect some S and N to have more or less skewed/even/diverse distributions. There is not an obvious rule-of-thumb to make generalizations about how different combinations of S and N will compare without sampling (or more staring at these plots). However, the feasible set is always dominated by hollow curves.
    4. Generally, I think it is better practice to compare an empirical SAD to its corresponding FS, and to draw our conclusions from how extreme or pedestrian the observed SAD is compared to its FS - than to try and glean too much about how the constraint envelope shifts with changing S and N. (.... writ large. I think it's still important to dig into how changing the number of cryptic rare species present would change the percentile values. But, importantly, that's not talking about driving S all the way up to equal a fixed N. That's about adding a couple of species with one individual each, increasing S and N simultaneously.). 
    5. Running up against small-FS-problems is confined to the very smallest communities. 